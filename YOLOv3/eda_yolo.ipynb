{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36964bitpytorchconda239caafb5f5f4ff1a355334659134c50",
   "display_name": "Python 3.6.9 64-bit ('pytorch': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract data from xml files into txt files\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "img_width = 504\n",
    "img_height = 747\n",
    "\n",
    "base_dir = 'data/custom/labels'\n",
    "len = 0\n",
    "for xml_file in os.listdir(base_dir):\n",
    "    if xml_file.endswith('xml'):\n",
    "        len += 1\n",
    "# print(len)\n",
    "\n",
    "for ind in range(len):\n",
    "    xml_file = f'{ind}.xml'\n",
    "    root = ET.parse(os.path.join(base_dir, xml_file)).getroot()\n",
    "    txt_file = f'{ind}.txt'\n",
    "\n",
    "    f = open(os.path.join(base_dir, txt_file), 'w+')\n",
    "    for object in root.findall('./object/bndbox'):\n",
    "        xmin = int(object[0].text)\n",
    "        ymin = int(object[1].text)\n",
    "        xmax = int(object[2].text)\n",
    "        ymax = int(object[3].text)\n",
    "\n",
    "        xcenter = ((xmin + xmax) / 2.0) / img_width\n",
    "        ycenter = ((ymin + ymax) / 2.0) / img_height\n",
    "        bbox_width = (xmax - xmin) / img_width\n",
    "        bbox_height = (ymax - ymin) / img_height\n",
    "\n",
    "        f.write(f'0 {xcenter} {ycenter} {bbox_width} {bbox_height}\\n')\n",
    "        print(f'0 {xcenter} {ycenter} {bbox_width} {bbox_height}')\n",
    "    f.close()\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "108\n{'type': 'net', 'batch': '16', 'subdivisions': '1', 'width': '416', 'height': '416', 'channels': '1', 'momentum': '0.9', 'decay': '0.0005', 'angle': '0', 'saturation': '1.5', 'exposure': '1.5', 'hue': '.1', 'learning_rate': '0.001', 'burn_in': '1000', 'max_batches': '500200', 'policy': 'steps', 'steps': '400000,450000', 'scales': '.1,.1'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '32', 'size': '3', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '64', 'size': '3', 'stride': '2', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '32', 'size': '1', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '64', 'size': '3', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'shortcut', 'from': '-3', 'activation': 'linear'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '128', 'size': '3', 'stride': '2', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '64', 'size': '1', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '128', 'size': '3', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'shortcut', 'from': '-3', 'activation': 'linear'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '64', 'size': '1', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '128', 'size': '3', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'shortcut', 'from': '-3', 'activation': 'linear'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '256', 'size': '3', 'stride': '2', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '128', 'size': '1', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '256', 'size': '3', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'shortcut', 'from': '-3', 'activation': 'linear'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '128', 'size': '1', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '256', 'size': '3', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'shortcut', 'from': '-3', 'activation': 'linear'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '128', 'size': '1', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '256', 'size': '3', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'shortcut', 'from': '-3', 'activation': 'linear'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '128', 'size': '1', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '256', 'size': '3', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'shortcut', 'from': '-3', 'activation': 'linear'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '128', 'size': '1', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '256', 'size': '3', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'shortcut', 'from': '-3', 'activation': 'linear'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '128', 'size': '1', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '256', 'size': '3', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'shortcut', 'from': '-3', 'activation': 'linear'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '128', 'size': '1', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '256', 'size': '3', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'shortcut', 'from': '-3', 'activation': 'linear'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '128', 'size': '1', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '256', 'size': '3', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'shortcut', 'from': '-3', 'activation': 'linear'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '512', 'size': '3', 'stride': '2', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '256', 'size': '1', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '512', 'size': '3', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'shortcut', 'from': '-3', 'activation': 'linear'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '256', 'size': '1', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '512', 'size': '3', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'shortcut', 'from': '-3', 'activation': 'linear'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '256', 'size': '1', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '512', 'size': '3', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'shortcut', 'from': '-3', 'activation': 'linear'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '256', 'size': '1', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '512', 'size': '3', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'shortcut', 'from': '-3', 'activation': 'linear'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '256', 'size': '1', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '512', 'size': '3', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'shortcut', 'from': '-3', 'activation': 'linear'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '256', 'size': '1', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '512', 'size': '3', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'shortcut', 'from': '-3', 'activation': 'linear'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '256', 'size': '1', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '512', 'size': '3', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'shortcut', 'from': '-3', 'activation': 'linear'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '256', 'size': '1', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '512', 'size': '3', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'shortcut', 'from': '-3', 'activation': 'linear'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '1024', 'size': '3', 'stride': '2', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '512', 'size': '1', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '1024', 'size': '3', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'shortcut', 'from': '-3', 'activation': 'linear'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '512', 'size': '1', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '1024', 'size': '3', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'shortcut', 'from': '-3', 'activation': 'linear'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '512', 'size': '1', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '1024', 'size': '3', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'shortcut', 'from': '-3', 'activation': 'linear'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '512', 'size': '1', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '1024', 'size': '3', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'shortcut', 'from': '-3', 'activation': 'linear'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '512', 'size': '1', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'size': '3', 'stride': '1', 'pad': '1', 'filters': '1024', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '512', 'size': '1', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'size': '3', 'stride': '1', 'pad': '1', 'filters': '1024', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '512', 'size': '1', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'size': '3', 'stride': '1', 'pad': '1', 'filters': '1024', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': 0, 'size': '1', 'stride': '1', 'pad': '1', 'filters': '18', 'activation': 'linear'}\n{'type': 'yolo', 'mask': '6,7,8', 'anchors': '10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326', 'classes': '1', 'num': '9', 'jitter': '.3', 'ignore_thresh': '.7', 'truth_thresh': '1', 'random': '1'}\n{'type': 'route', 'layers': '-4'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '256', 'size': '1', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'upsample', 'stride': '2'}\n{'type': 'route', 'layers': '-1, 61'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '256', 'size': '1', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'size': '3', 'stride': '1', 'pad': '1', 'filters': '512', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '256', 'size': '1', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'size': '3', 'stride': '1', 'pad': '1', 'filters': '512', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '256', 'size': '1', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'size': '3', 'stride': '1', 'pad': '1', 'filters': '512', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': 0, 'size': '1', 'stride': '1', 'pad': '1', 'filters': '18', 'activation': 'linear'}\n{'type': 'yolo', 'mask': '3,4,5', 'anchors': '10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326', 'classes': '1', 'num': '9', 'jitter': '.3', 'ignore_thresh': '.7', 'truth_thresh': '1', 'random': '1'}\n{'type': 'route', 'layers': '-4'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '128', 'size': '1', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'upsample', 'stride': '2'}\n{'type': 'route', 'layers': '-1, 36'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '128', 'size': '1', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'size': '3', 'stride': '1', 'pad': '1', 'filters': '256', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '128', 'size': '1', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'size': '3', 'stride': '1', 'pad': '1', 'filters': '256', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'filters': '128', 'size': '1', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': '1', 'size': '3', 'stride': '1', 'pad': '1', 'filters': '256', 'activation': 'leaky'}\n{'type': 'convolutional', 'batch_normalize': 0, 'size': '1', 'stride': '1', 'pad': '1', 'filters': '18', 'activation': 'linear'}\n{'type': 'yolo', 'mask': '0,1,2', 'anchors': '10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326', 'classes': '1', 'num': '9', 'jitter': '.3', 'ignore_thresh': '.7', 'truth_thresh': '1', 'random': '1'}\nphantom_20\nclasses = 1\ntrain = data/custom/train_valid/phantom_20/train.txt\nvalid = data/custom/train_valid/phantom_20/valid.txt\nnames = data/custom/classes.names\ninvivo_91\nclasses = 1\ntrain = data/custom/train_valid/invivo_91/train.txt\nvalid = data/custom/train_valid/invivo_91/valid.txt\nnames = data/custom/classes.names\n"
    },
    {
     "data": {
      "text/plain": "{'classes': '1',\n 'train': 'data/custom/train_valid/invivo_91/train.txt',\n 'valid': 'data/custom/train_valid/invivo_91/valid.txt',\n 'names': 'data/custom/classes.names'}"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### test written functions - model/data confie parsing\n",
    "from utils import parse_model_config, load_classes, parse_data_config\n",
    "\n",
    "# test load_classes\n",
    "classes = load_classes('data/custom/classes.names')\n",
    "\n",
    "# test parse_model_config\n",
    "module_defs = parse_model_config('config/yolov3-custom.cfg')\n",
    "print(f'{len(module_defs)}')\n",
    "for item in module_defs:\n",
    "    # if item['type'] == 'yolo':\n",
    "    #     print(item)\n",
    "    print(item)\n",
    "\n",
    "# test parse_data_config\n",
    "parse_data_config('invivo_91', 'config/custom.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['data/custom/images/phantom_20/0.jpg', 'data/custom/images/phantom_20/1.jpg', 'data/custom/images/phantom_20/10.jpg', 'data/custom/images/phantom_20/11.jpg', 'data/custom/images/phantom_20/13.jpg', 'data/custom/images/phantom_20/14.jpg', 'data/custom/images/phantom_20/15.jpg', 'data/custom/images/phantom_20/16.jpg', 'data/custom/images/phantom_20/18.jpg', 'data/custom/images/phantom_20/19.jpg', 'data/custom/images/phantom_20/2.jpg', 'data/custom/images/phantom_20/3.jpg', 'data/custom/images/phantom_20/6.jpg', 'data/custom/images/phantom_20/7.jpg', 'data/custom/images/phantom_20/8.jpg', 'data/custom/images/phantom_20/9.jpg']\n[]\n"
    }
   ],
   "source": [
    "dataset_name = 'phantom_20'\n",
    "with open('data/custom/train_valid/phantom_20/train.txt') as file:\n",
    "    img_files = [f'data/custom/images/{dataset_name}/' + x.strip() for x in file.readlines()]\n",
    "    label_files = [f'data/custom/labels/{dataset_name}/' + x.strip()[:-4] + '.txt' for x in file.readlines()]\n",
    "    print(img_files)\n",
    "    print(label_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand image preprocessing pipeline\n",
    "from utils.datasets import pad_to_square\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "path = 'data/custom/images/0.jpg'\n",
    "img = Image.open(path)\n",
    "img_np = np.array(Image.open(path))\n",
    "img_tensor = transforms.ToTensor()(img.convert('RGB'))\n",
    "\n",
    "print(img.size)\n",
    "print(img_np.shape)\n",
    "print(img_tensor.shape)\n",
    "\n",
    "img_padded, pad = pad_to_square(img_tensor, 0)\n",
    "print(img_padded.size())\n",
    "print(pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand YOLO layer Implementation\n",
    "grid_x = torch.arange(9).repeat(9, 1).view([1,1,9,9]).type(torch.FloatTensor)\n",
    "grid_y = torch.arange(9).repeat(9, 1).t().view([1,1,9,9]).type(torch.FloatTensor)\n",
    "print(grid_x)\n",
    "print(grid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\"Metrics\", *[f\"YOLO Layer {i}\" for i in range(5)]]\n",
    "\n",
    "metrics = [\"grid_size\", \"loss\", \"x\", \"y\", \"w\", \"h\", \"conf\", \"cls\", \"cls_acc\", \"recall50\", \"recall75\", \"precision\", \"conf_obj\", \"conf_noobj\"]\n",
    "{m: \"%.6f\" for m in metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Prediction on Single Image\n",
    "from yolo_model import Darknet\n",
    "from datasets import pad_to_square, resize\n",
    "from utils import non_max_suppression, rescale_boxes, load_classes\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.ticker import NullLocator\n",
    "\n",
    "# model set up\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Darknet('config/yolov3-custom.cfg', img_size=416).to(device)\n",
    "model.load_state_dict(torch.load('checkpoints/yolov3_ckpt_38.pth'))\n",
    "model.eval()\n",
    "\n",
    "# image preprocessing\n",
    "img_path = 'data/custom/images/0.jpg'\n",
    "img = transforms.ToTensor()(Image.open(img_path))\n",
    "img, _ = pad_to_square(img, 0)\n",
    "img = resize(img, 416).unsqueeze(0)\n",
    "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "img_tensor = Variable(img.type(Tensor))\n",
    "\n",
    "# model prediction\n",
    "with torch.no_grad():\n",
    "    detections = model(img_tensor)\n",
    "    detections = non_max_suppression(detections, conf_thres=0.85, nms_thres=0.35)\n",
    "# print(f\"detections: {detections[0]}\")\n",
    "\n",
    "# result visualization\n",
    "img_np = np.array(Image.open(img_path))\n",
    "detections = rescale_boxes(detections[0], 416, img_np.shape[:2])\n",
    "# print(f\"rescaled detections: {detections}\")\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(12,8))\n",
    "ax.imshow(img_np, cmap='gray')\n",
    "cmap = plt.get_cmap(\"tab20b\")\n",
    "classes = load_classes('data/custom/classes.names')\n",
    "for x1, y1, x2, y2, obj_conf, cls_conf, cls_pred in detections:\n",
    "    # print(\"\\t+ Label: %s, obj_conf: %.5f, cls_conf: %.5f\" % (classes[int(cls_pred)], obj_conf.item(), cls_conf.item()))\n",
    "    x1_draw = max(x1, 2)\n",
    "    y1_draw = max(y1, 2)\n",
    "    x2_draw = min(x2, img_np.shape[1] - 2)\n",
    "    y2_draw = min(y2, img_np.shape[0] - 2)\n",
    "\n",
    "    color = cmap(13)\n",
    "    bbox = patches.Rectangle((x1_draw, y1_draw), x2_draw - x1_draw, y2_draw - y1_draw, linewidth=2, edgecolor=color, facecolor=\"none\") # create a rectangle patch\n",
    "    ax.add_patch(bbox) # add bbox to the plot\n",
    "    # add label\n",
    "    if x1 > 0:\n",
    "        plt.text(x1, y1, s=classes[int(cls_pred)], color=\"white\", verticalalignment=\"bottom\", horizontalalignment='left', bbox={\"color\": color, \"pad\": 0})\n",
    "    elif x2 < img_np.shape[1]:\n",
    "        plt.text(x2, y2, s=classes[int(cls_pred)], color=\"white\", verticalalignment=\"top\", horizontalalignment='right', bbox={\"color\": color, \"pad\": 0})\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.gca().xaxis.set_major_locator(NullLocator())\n",
    "plt.gca().yaxis.set_major_locator(NullLocator())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Comparison between input_img and output_img\n",
    "input_img = Image.open('data/custom/images/0.jpg').convert('RGB')\n",
    "output_img = Image.open('output/0.png')\n",
    "\n",
    "imgs = [input_img, output_img]\n",
    "titles = [f'input_img: {input_img.size}', f'output_img: {output_img.size}']\n",
    "plt.figure(figsize=(12,8))\n",
    "for i, (img, title) in enumerate(zip(imgs, titles)):\n",
    "    plt.subplot(1, 2, i + 1)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}